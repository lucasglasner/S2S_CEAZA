{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> This notebook has the postprocess of the raw downloaded S2S forecast members and the following construction of the coastnorth/coastal hovmoller dataset\n",
    "\n",
    "The S2S data where download for the pacific basin (110°E - 295°E, 45°S, 45°N), including variables in the \"surface\" category and in the \"ocean category\". The main difference is that the surface variables (winds and sst) are in a global 1.5°x1.5° grid, and the ocean variables (sea level, heat content, etc) are in a 1°x1° grid. Since this last grid is the finer one, the postprocess includes a regridding of the surface variables to the ocean grid, where for all variables a bilinear interpolation will be the main inteprolation method. In addition, 11 members are going to be used (1 control and 10 perturbation forecasts), so the structure of the new dataset will have the following dimensions: distance along the hovmoller (space), initialization time since 2000 to 2022, leadtime (from 0 to 46) and ensame member (from 0 to 10, being 0 the control simulation). Lets get started!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-25 15:50:57,286 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ngjcc2mf', purging\n",
      "2023-07-25 15:50:57,286 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-3l5j2s9u', purging\n",
      "2023-07-25 15:50:57,286 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-erooqlp1', purging\n",
      "2023-07-25 15:50:57,286 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-v4wxsg5a', purging\n",
      "2023-07-25 15:50:57,286 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-w117m19a', purging\n",
      "2023-07-25 15:50:57,286 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-aknl2qe_', purging\n",
      "2023-07-25 15:50:57,287 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-3sth9mw7', purging\n",
      "2023-07-25 15:50:57,287 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-9u103joy', purging\n",
      "2023-07-25 15:50:57,287 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-abw9drsd', purging\n",
      "2023-07-25 15:50:57,287 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-wv6sfetf', purging\n",
      "2023-07-25 15:50:57,287 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-6yhx4tf2', purging\n",
      "2023-07-25 15:50:57,287 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-zv6vnh3x', purging\n",
      "2023-07-25 15:50:57,287 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-5cf3mtf5', purging\n",
      "2023-07-25 15:50:57,287 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-mpvtkdaz', purging\n",
      "2023-07-25 15:50:57,288 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-5o9jb27g', purging\n",
      "2023-07-25 15:50:57,288 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-r3fmeu42', purging\n",
      "2023-07-25 15:50:57,288 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-7olo1w4p', purging\n",
      "2023-07-25 15:50:57,288 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ct_15xc_', purging\n",
      "2023-07-25 15:50:57,288 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-g27dgruk', purging\n",
      "2023-07-25 15:50:57,288 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ddvsnqvj', purging\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import xarray as xr\n",
    "import xesmf as xe\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Just for supressing an annoying warning\n",
    "import dask\n",
    "dask.config.set({\"array.slicing.split_large_chunks\": False})\n",
    "\n",
    "# For using more cores\n",
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster(n_workers=20, threads_per_worker=1)\n",
    "client  = Client(cluster, asynchronous=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load masks\n",
    "mask = xr.open_dataset('data/S2S_masks.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backup\t   ensamble1   ensamble2  ensamble4  ensamble6\tensamble8\n",
      "ensamble0  ensamble10  ensamble3  ensamble5  ensamble7\tensamble9\n",
      "\n",
      "\n",
      "2020-01-06\n",
      "2020-01-09\n",
      "2020-01-13\n",
      "2020-01-16\n",
      "2020-01-20\n",
      "\n",
      "\n",
      "2000-01-06_O2D.nc\n",
      "2000-01-06_SURF.nc\n",
      "2000-01-06_TSM.nc\n",
      "2001-01-06_O2D.nc\n",
      "2001-01-06_SURF.nc\n"
     ]
    }
   ],
   "source": [
    "!ls data/S2S/REFORECASTS/\n",
    "print('\\n')\n",
    "!ls data/S2S/REFORECASTS/ensamble5 | head -n 5\n",
    "print('\\n')\n",
    "!ls data/S2S/REFORECASTS/ensamble5/2020-01-06 | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen in the previous shell command the reforecasts dataset is stored in different folders. First of all there is a folder with the corresponding ensamble member. Second there is a folder with the initialization time of each near real time (NRT) S2S forecast. Third and final are the netcdf files of the corresponding reforecast of the NRT forecast. The reforecasts are forecast initialized in the same day as a NRT S2S forecast but for the previous 20 years. Its strange but it is what it is. The dataset is saved on disk with this format because of how S2S is run (monday and thursday of every week) exists the possibility of repeated reforecasts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_preprocess(ds):\n",
    "    \"\"\"\n",
    "    This small function open an S2S netcdf file and grab the \n",
    "    46 leadtimes and define the leadtime and inittime coordinates\n",
    "    \"\"\"\n",
    "    ds = ds.squeeze().isel(time=slice(-46,None))\n",
    "    ds = ds.assign_coords({'inittime':ds.time[0].values})\n",
    "    ds = ds.rename({'time':'leadtime'})\n",
    "    ds.coords['leadtime'] = ('leadtime',np.arange(len(ds.leadtime)))\n",
    "    return ds.compute()\n",
    "\n",
    "def preprocess_O2D(ds):\n",
    "    \"\"\"\n",
    "    For the ocean downloaded variables just make some clean up\n",
    "    \"\"\"\n",
    "    ds = first_preprocess(ds)\n",
    "    ds = ds.drop(['depth','depth_2','depth_2_bnds'])\n",
    "    ds = ds.rename({'dslm':'zos','param18.4.10':'T300'})\n",
    "    return ds\n",
    "\n",
    "def preprocess_SURF(ds):\n",
    "    \"\"\"\n",
    "    For downloaded winds just make some clean up\n",
    "    \"\"\"\n",
    "    ds = first_preprocess(ds)\n",
    "    ds = ds.drop(['height'])\n",
    "    ds = ds[['10u','10v']]\n",
    "    return ds\n",
    "\n",
    "def load_s2s(member, date):\n",
    "    \"\"\"\n",
    "    Given the ensamble and NRT S2S date this function\n",
    "    loads all the S2S data as an xarray (ocean and winds)\n",
    "    \"\"\"\n",
    "    path_surf = glob(f'data/S2S/REFORECASTS/ensamble{member}/{date}/*_SURF.nc')\n",
    "    path_sst  = glob(f'data/S2S/REFORECASTS/ensamble{member}/{date}/*_TSM.nc')\n",
    "    path_O2D  = glob(f'data/S2S/REFORECASTS/ensamble{member}/{date}/*_O2D.nc')\n",
    "    # Load surface and ocean datasets\n",
    "    surf = xr.merge([xr.open_mfdataset(path_surf, preprocess=preprocess_SURF,parallel=True, concat_dim='inittime', combine='nested'),\n",
    "                    xr.open_mfdataset(path_sst, preprocess=first_preprocess,parallel=True, concat_dim='inittime', combine='nested')])\n",
    "    o2d  = xr.open_mfdataset(path_O2D, preprocess=preprocess_O2D,parallel=True, concat_dim='inittime', combine='nested')\n",
    "    # Perform the regridding of 1.5°x1.5° surface to 1°x1° ocean\n",
    "    regridder = xe.Regridder(surf,o2d,'bilinear')\n",
    "    surf      = regridder(surf.ffill('lon'))\n",
    "    # Merge everything\n",
    "    s2s = xr.merge([surf,o2d])\n",
    "    s2s['sst'] = s2s['sst'].where(~np.isnan(s2s.zos))\n",
    "    s2s = s2s.sel(lon=slice(111,293))\n",
    "    del surf, o2d\n",
    "    return s2s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
